<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="In the learning phase a set of examples are shown to the classifier, with class labels (supervised learning) or without them (unsupervised learning). As a result, the classifier is then able to catego">
<meta property="og:type" content="article">
<meta property="og:title" content="Building a machine-learning pipeline with scikit-learn and Qt - Part VI">
<meta property="og:url" content="https://alexpnt.github.io/2017/09/10/ml-pipeline-6/index.html">
<meta property="og:site_name" content="Alexandre Pinto">
<meta property="og:description" content="In the learning phase a set of examples are shown to the classifier, with class labels (supervised learning) or without them (unsupervised learning). As a result, the classifier is then able to catego">
<meta property="og:locale">
<meta property="og:image" content="https://alexpnt.github.io/images/ml-pipeline/metrics.png">
<meta property="og:image" content="https://alexpnt.github.io/images/ml-pipeline/roc.png">
<meta property="og:image" content="https://alexpnt.github.io/images/ml-pipeline/precision-recall.png">
<meta property="article:published_time" content="2017-09-10T09:30:41.000Z">
<meta property="article:modified_time" content="2020-08-22T15:04:57.159Z">
<meta property="article:author" content="Alexandre Pinto">
<meta property="article:tag" content="python">
<meta property="article:tag" content="machine-learning">
<meta property="article:tag" content="scikit-learn">
<meta property="article:tag" content="qt5">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://alexpnt.github.io/images/ml-pipeline/metrics.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Building a machine-learning pipeline with scikit-learn and Qt - Part VI</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- rss -->
    
    
<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/rss.xml" title="Alexandre Pinto" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/cv/">CV</a></li>
         
          <li><a href="/blog/">Blog</a></li>
         
          <li><a href="/papers/">Papers</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/01/06/fast-inference-falcon-bjoern/"><i class="fa fa-chevron-left" aria-hidden="true" onmouseover='$("#i-prev").toggle();' onmouseout='$("#i-prev").toggle();'></i></a></li>
        
        
        <li><a class="icon" href="/2017/09/09/ml-pipeline-5/"><i class="fa fa-chevron-right" aria-hidden="true" onmouseover='$("#i-next").toggle();' onmouseout='$("#i-next").toggle();'></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up" aria-hidden="true" onmouseover='$("#i-top").toggle();' onmouseout='$("#i-top").toggle();'></i></a></li>
        <li><a class="icon" href="#"><i class="fa fa-share-alt" aria-hidden="true" onmouseover='$("#i-share").toggle();' onmouseout='$("#i-share").toggle();' onclick='$("#share").toggle();return false;'></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/"><i class="fa fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&text=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&is_video=false&description=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Building a machine-learning pipeline with scikit-learn and Qt - Part VI&body=Check out this article: https://alexpnt.github.io/2017/09/10/ml-pipeline-6/"><i class="fa fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&name=Building a machine-learning pipeline with scikit-learn and Qt - Part VI&description="><i class="fa fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#classifiers"><span class="toc-number">1.</span> <span class="toc-text">Classifiers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#evaluation-metrics"><span class="toc-number">2.</span> <span class="toc-text">Evaluation Metrics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#visualizing-the-performance"><span class="toc-number">3.</span> <span class="toc-text">Visualizing the performance</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index width mx-auto px2 my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Building a machine-learning pipeline with scikit-learn and Qt - Part VI
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Alexandre Pinto</span>
      </span>
      
    <div class="postdate">
        <time datetime="2017-09-10T09:30:41.000Z" itemprop="datePublished">2017-09-10</time>
    </div>


      
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/machine-learning/" rel="tag">machine-learning</a>, <a class="tag-link-link" href="/tags/python/" rel="tag">python</a>, <a class="tag-link-link" href="/tags/qt5/" rel="tag">qt5</a>, <a class="tag-link-link" href="/tags/scikit-learn/" rel="tag">scikit-learn</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>In the learning phase a set of examples are shown to the classifier, with class labels (supervised learning) or without them (unsupervised learning). As a result, the classifier is then able to categorize, with a certain accuracy, new unseen data. Experimental evaluation is also an important step to assess the effectiveness of a classifier, i.e, the quality of the decisions made by a predictive model on unseen data. This post is a follow up from the <a href="/2017/09/09/ml-pipeline-5/">previous post</a> and describes some classical and well-known learning methods available and how to evaluate their performance.</p>
<h2 id="classifiers">Classifiers</h2>
<p>Some common learning algorithms are described bellow:</p>
<ul>
<li><p><strong>Minimum Distance Classifier</strong> - Minimum Distance Classifiers are template matching system where unseen feature vectors are matched against template prototypes (vectors) representing the classes. This matching usually involves computing the matching error between both feature vectors using for example the euclidean distance as the error metric. This matching distance can simply be stated as: <span class="math display">\[ || x - m_k|| \]</span> where <em>x</em> is the unseen feature vector and <em>m<sub>k</sub></em> is the feature prototype for class <em>k</em>, usually consisting in a feature vector with feature mean values obtained during the training phase. After <em>k</em> tests are performed, the class belonging to the prototype with the minimum distance is chosen.</p></li>
<li><p><strong>k-Nearest Neighbor (kNN)</strong> - The kNN is a simple and fast unsupervised algorithm that classifies unknown instances based on the majority class from <em>k</em> neighbors. This method starts by constructing a distance matrix between the training samples and the new test sample and chooses the nearest <em>k</em> neighbors. Afterwards, the majority label from these neighbors is assigned to the new data. For two-class problems, <em>k</em> is usually an odd number to prevent ties.</p></li>
<li><p><strong>Naive Bayes</strong> - The Naive Bayes Classifier is a simple supervised probabilistic model based on the Bayes’ theorem. The posterior probabilities are computed for each class w<sub>j</sub> and the class with largest outcome is chosen to classify the feature vector <em>x</em>. To achieve this result, the likelihood, prior and evidence probabilities must be calculated, as shown below: <span class="math display">\[ posterior \, probability =  { likelihood \times prior \; probability \over evidence} \]</span> <span class="math display">\[ P(w_j | x) =  { p(x|w_j) \times P(w_j)  \over  p(x) } \]</span> The main difficulty is to compute the likelihoods <em>p(x|ω<sub>j</sub>)</em> since the other factors are obtained from the data. <em>P(ω<sub>j</sub>)</em> are the prior probabilities and <em>p(x)</em> is a normalization factor which is sometimes omitted. Assuming independence of the features, the likelihoods can be obtained as shown bellow: <span class="math display">\[ P(x | w_j) =  { \prod P(x_k | w_j) } \]</span> This simplification allows to compute the class conditional probabilities for each feature separately, reducing complexity and computational costs.</p></li>
<li><p><strong>Support Vector Machines (SVM)</strong> - Support vector machines are optimization methods for binary classification tasks that map the features to a higher dimensional space where an optimal linear hyperplane that separates the existing classes exists. The decision boundary is chosen with the help of some training examples, called the support vectors, that have the widest separation between them and help maximizing the margin between the boundaries of the different classes. The decision surface is in the “middle” of these vectors. During the training phase, the coefficient vector <em>w</em> and the constant <em>b</em> that define the separating hyperplane are searched such that the following error function is minimized: <span class="math display">\[ minimize: {1 \over 2} ||w||^2 + C \sum ξ_i \]</span> <span class="math display">\[  s.t. \quad y_i(φ(x_i) + b) \geq 1 - ξ_i \]</span> where <em>C</em> is an arbitrary constant and <em>ξ</em> are slack variables used to penalize misclassified instances that increase with the distance from the margin. If <em>C</em> is large, the penalty for misclassification is greater. The vectors <em>x<sub>i</sub></em> are the training instances. This method makes use of a kernel function <em>φ</em> used to transform the input vectors into higher dimensional vectors. New instances are then classified according to which ”side” of the hyperplane they are located.</p></li>
<li><p><strong>Decision Tree</strong> - Decision Trees are supervised methods that learn rules based on the training data to classify new instances. The built trees are simple to understand and visualize. During training, each node of the tree is recursively split by the feature that provides, for example, the best information gain.</p></li>
<li><p><strong>Random Forest</strong> - Random forests are ensemble methods, i.e., they take into consideration the prediction of several classifiers in order to improve the accuracy and robustness of the prediction results. In the case of random forests, they train a set of random trees with bootstrapped samples (samples drawn with replacement) from the original training data. Each tree is grown by selecting m random features from the d features and recursively splitting the data by the best splits. The classification of new data is achieved by a majority vote.</p></li>
</ul>
<p>Each classifier has its own advantages and disadvantages and the right technique should be chosen according to the requirements of the problem-at-hand.</p>
<h2 id="evaluation-metrics">Evaluation Metrics</h2>
<p>In classification tasks, predictions made by a classifier are either considered Positive or False (under some category) and the expected judgments are called True or False (again, under a certain category). Common metrics are:</p>
<ul>
<li><p><strong>Accuracy</strong> - This measure provides a proportion of correctly classified instances and correctly rejected instances (True Positives and True Negatives) among the whole dataset. <span class="math display">\[ Acc = {TP + TN \over TP + TN + FP + FN}\]</span></p></li>
<li><p><strong>Precision</strong> - This measure provides a proportion of correctly classified instances (True Positives) among all the positive identified instances (True Positives and False Positives). <span class="math display">\[ P = {TP \over TP + FP }\]</span></p></li>
<li><p><strong>Recall</strong> - This measure, sometimes called sensitivity, provides a proportion of correctly classified instances (True Positives) among the positive instances that were and should have been correctly identified, i.e., the whole positive part of the dataset (True Positives and False Negatives). <span class="math display">\[ R = {TP \over TP + FN }\]</span></p></li>
<li><p><strong>F-measure</strong> - This measure combines precision and recall and provides a balance between them. It is computed as the harmonic mean between the two metrics providing the same weight for both. <span class="math display">\[ F_1 = {2 \times P \times R \over P +  R }\]</span></p></li>
<li><p><strong>Stratified K-fold Cross Validation</strong> - A popular technique to evaluate the performance of the system is to split the data into training and testing sets, using the later to estimate the true generalization performance of the classifier. However, this may bring some issues such as the trade-offs between the percentage splits or the representativity of the test set. A popular accepted approach is to split the entire dataset into k representative partitions, using k − 1 of these partitions for training and the remaining one for testing. This process is then repeated <em>k</em> times, (each time using a different test partition) and the results averaged.</p></li>
</ul>
<p><img src="/images/ml-pipeline/metrics.png"></p>
<h2 id="visualizing-the-performance">Visualizing the performance</h2>
<ul>
<li><strong>Receiver Operating Characteristics (ROC)</strong> - ROC curves are useful graphs to visualize the performance of binary classifiers. They are useful to compare the rates at which the classifier is making correct predictions (True Positive Rate plotted on the Y axis) against the rate of false alarms (False Positive Rate plotted on the X axis). Important points in this graph are the lower left point (0,0), representing a classifier that never classify positive instances, neither having False Positives or True Positives. On the other hand, the upper right point (1,1) represents a classifier that classifies every instance as positive, disregarding if it is a false positive or not. Finally, the point (0,1) represents the perfect classifier, where every instance was correctly classified. The area bellow the ROC curve is called Area Under the Curve (AUC) and is also a good evaluation measure. A perfect classifier would have an AUC of 1.0 while a random classifier would only have 0.5.</li>
</ul>
<p><img src="/images/ml-pipeline/roc.png"></p>
<ul>
<li><strong>Precision-Recall Curves (PR)</strong> - The Precision-Recall Curve plots the trade-off between the precision and recall achieved by a classifier, by showing the recall on the X axis and precision on the Y axis. An important point in this graph is the upper right point (1,1) which represents the ideal classifier having maximum precision and recall. Figure 2.4 shows three hypothetical classifiers and the areas of good and bad performance, which are above or below the line defined by a random classifier. The area bellow the PR curve is called Average Precision (AP) and is also a good measure. A perfect classifier would have an Average Precision of 1.0 while a random classifier would only have 0.5.</li>
</ul>
<p><img src="/images/ml-pipeline/precision-recall.png"></p>
<p>In this series of posts, a machine learning application was presented, showing a typical pipeline with the following steps: * Dataset Loading * Feature Assessment where the distribution of features are inspected and visualized * Preprocessing where data transformations are applied and issues such as dataset balancing are addressed. * Feature Selection methods where features with he most discriminative power are selected * Classifications performed by predictive models where their performance is evaluated using ROC curves, Precision-Recall Curves and generic metrics such as Accuracy, Precision, Recall and F1 measures.</p>
<p>The machine learning method is an iterative process, where one needs to go back and forth, setting different parameters, and different methods in order to see which combination works best and provides the best results. You can checkout this application and try it for yourself. The source code is available in <a href="https://github.com/AlexPnt/Default-Credit-Card-Prediction" target="_blank" rel="noopener">this repository</a>.</p>

  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        </div>
    </div>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/cv/">CV</a></li>
         
          <li><a href="/blog/">Blog</a></li>
         
          <li><a href="/papers/">Papers</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#classifiers"><span class="toc-number">1.</span> <span class="toc-text">Classifiers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#evaluation-metrics"><span class="toc-number">2.</span> <span class="toc-text">Evaluation Metrics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#visualizing-the-performance"><span class="toc-number">3.</span> <span class="toc-text">Visualizing the performance</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/"><i class="fa fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&text=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&is_video=false&description=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Building a machine-learning pipeline with scikit-learn and Qt - Part VI&body=Check out this article: https://alexpnt.github.io/2017/09/10/ml-pipeline-6/"><i class="fa fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&title=Building a machine-learning pipeline with scikit-learn and Qt - Part VI"><i class="fa fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://alexpnt.github.io/2017/09/10/ml-pipeline-6/&name=Building a machine-learning pipeline with scikit-learn and Qt - Part VI&description="><i class="fa fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
      <ul>
        <li id="toc"><a class="icon" href="#" onclick='$("#toc-footer").toggle();return false;'><i class="fa fa-list fa-lg" aria-hidden="true"></i> TOC</a></li>
        <li id="share"><a class="icon" href="#" onclick='$("#share-footer").toggle();return false;'><i class="fa fa-share-alt fa-lg" aria-hidden="true"></i> Share</a></li>
        <li id="top" style="display:none"><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a></li>
        <li id="menu"><a class="icon" href="#" onclick='$("#nav-footer").toggle();return false;'><i class="fa fa-bars fa-lg" aria-hidden="true"></i> Menu</a></li>
      </ul>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 Alexandre Pinto
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/cv/">CV</a></li>
         
          <li><a href="/blog/">Blog</a></li>
         
          <li><a href="/papers/">Papers</a></li>
        
      </ul>
    </nav>
  </div>
</footer><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
<!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<link rel="stylesheet" href="/lib/meslo-LG/styles.css">


<link rel="stylesheet" href="/lib/justified-gallery/justifiedGallery.min.css">



<!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/jquery.justifiedGallery.min.js"></script>


<script src="/js/main.js"></script>

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-43571022-2', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'alexpnt';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

